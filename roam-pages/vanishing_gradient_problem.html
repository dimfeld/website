---
title: "Vanishing Gradient Problem"
tags: Machine Learning
date: 2021-02-12
updated: 2021-02-12
---


  <ul class="list-bullet">
    <li id="mg_8PKfrp">When training a neural network and the derivative of the error function is too small, it can effectively stop updating, or make extremely slow progress.</li>
    <li id="xiBgOVQtu">This causes training to finish slowly or not at all.</li>
    <li id="GPmoExd3x">This is a problem sometimes seen with <a class="block-ref" href="/notes/activation_functions#hr7Uxi9XY">Sigmoid</a> activation functions, among others, due to their saturating nature at the extremes.</li>
    <li id="wPcY21gvQ">Faster hardware has helped with this issue, and other types of functions such as <a class="block-ref" href="/notes/activation_functions#MUcAWcI3S">ReLU</a> help as well.</li>
  </ul>


