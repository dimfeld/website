---
title: "Vanishing Gradient Problem"
tags: Machine Learning
date: 2021-02-12
updated: 2021-02-12
---


  <ul class="list-bullet">
    <li>When training a neural network and the derivative of the error function is too small, it can effectively stop updating, or make extremely slow progress.</li>
    <li>This causes training to finish slowly or not at all.</li>
    <li>This is a problem sometimes seen with <a class="block-ref" href="/notes/activation_functions#f7bcdbb3-dc51-4996-9ed5-9ae478763eaf">Sigmoid</a> activation functions, among others, due to their saturating nature at the extremes.</li>
    <li>Faster hardware has helped with this issue, and other types of functions such as <a class="block-ref" href="/notes/activation_functions#c0c20b2a-4a97-463c-8888-a3ae200c8506">ReLU</a> help as well.</li>
  </ul>


