---
title: "Smelter"
tags: Projects
date: 2023-06-27
updated: 2024-03-23
---


  <ul class="list-bullet">
    <li>This is a project to do map-reduce style calculations using a bunch of serverless workers reading from S3.</li>
    <li><h2>Task List</h2>
      <ul class="list-bullet">
        <li><h3>Up Next</h3></li>
        <li><h3>Soon</h3></li>
        <li><h3>Later/Maybe</h3>
          <ul class="list-bullet">
            <li><input type="checkbox" disabled  /> Platform adapters could make it easier to build the local and platform-side code
              <ul class="list-bullet">
                <li>Not sure how useful this actually is, but this would mostly be a preconfigured way for a binary to determine if its running in the platform or not and then run your manager code or your worker code when you&#39;re doing the single-source-code configuration</li>
                <li>I think a simple abstraction would be something like:
                  <ul class="list-bullet">
                    <li>The spawner has an easy way to set a &quot;SMELTER_TASK&quot; variable in the environment (or whatever is the equivalent) of the spawned task.</li>
                    <li>Provide a function that retrieves this value, and then you can match on it and run your own code. If it&#39;s not set, then you assume that you are the job manager and run that code instead.</li>
                    <li>Ultimately this isn&#39;t too different from doing it yourself, might be a nice convenience though especially when developing the pipeline.</li>
                  </ul>                </li>
              </ul>            </li>
            <li><input type="checkbox" disabled  /> Ability for spawner to send an event when a container goes from pending to running</li>
            <li><input type="checkbox" disabled  /> Option to run a server within the worker container that can collect logs and stream them back to the manager</li>
            <li><input type="checkbox" disabled  /> Option to timeout if we don&#39;t see any logs for too long
              <ul class="list-bullet">
                <li>This depends on being able to get logs shipped from the container down to the manager.</li>
              </ul>            </li>
            <li><input type="checkbox" disabled  /> AWS Lambda adapter</li>
            <li><input type="checkbox" disabled  /> Support Fargate spot instances
              <ul class="list-bullet">
                <li>A way for SpawnedTask to indicate that a task failure is from a spot instance shutdown
                  <ul class="list-bullet">
                    <li>This can be a new error code alongside <code>TaskFailed</code>, call it <code>TaskPreempted</code></li>
                  </ul>                </li>
                <li>Specify which capacity provider to use for Spot vs. Normal launching</li>
                <li>An option to launch it as a non-spot instance if the spot instance gets preempted too much or if it takes too long to start.</li>
                <li>Currently Fargate doesn&#39;t support spot on ARM</li>
              </ul>            </li>
            <li><input type="checkbox" disabled  /> Convert a single SQL statement into a multi-stage set of tasks
              <ul class="list-bullet">
                <li>This may be better done by integrating with something like <a href="https://arrow.apache.org/datafusion/">Datafusion</a> which already supports creating query plans</li>
                <li>Consider also supporting <a href="https://substrait.io/">Substrait</a> query plan format</li>
              </ul>            </li>
            <li><input type="checkbox" disabled  /> Clean up inter-level channels
              <ul class="list-bullet">
                <li>There are a bunch of channels for communicating cancel state, task results, etc. between the tasks, stages, job, and manager. See if there&#39;s a way to make this cleaner.</li>
              </ul>            </li>
          </ul>        </li>
        <li><h3>Done</h3>
          <ul class="list-bullet">
            <li><input type="checkbox" disabled checked /> Retry inside the spawner when Fargate returns a capacity error</li>
            <li><input type="checkbox" disabled checked /> Cancel all the jobs when an error occurs, and wait for them to be cancelled</li>
            <li><input type="checkbox" disabled checked /> Handle rate exceeded errors from Fargate API
              <ul class="list-bullet">
                <li>The Rust AWS SDK doesn&#39;t properly handle <code>ThrottlingError</code> and flag it for retry, but with a custom retry layer this can be done.</li>
              </ul>            </li>
            <li><input type="checkbox" disabled checked /> Workers automatically track RAM usage, load factor, time running, etc., and return these with the task results</li>
            <li><input type="checkbox" disabled checked /> Full test of Fargate jobs</li>
            <li><input type="checkbox" disabled checked /> Allow a cancel signal to kill all containers</li>
            <li><input type="checkbox" disabled checked /> StatusSender needs to work with a raw StatusUpdateItem, and the StatusCollector should be able to handle native StatusUpdateItem and have its own separate mechanism for the read operations. (Maybe just use a mutex here to simplify?)</li>
            <li><input type="checkbox" disabled checked /> Build fargate container</li>
            <li><input type="checkbox" disabled checked /> Have something that helps generate task definitions for fargate</li>
            <li><input type="checkbox" disabled checked /> AWS Fargate Spawner
              <ul class="list-bullet">
                <li>How to figure out if a container succeeded or not? &mdash; exit code is exposed though there&#39;s no other way to pass back data</li>
              </ul>            </li>
            <li><input type="checkbox" disabled checked /> AWS Fargate Worker Framework</li>
            <li id="651362f2-b0ec-4bf6-b14a-226b2f8cc350"><input type="checkbox" disabled checked /> Example to run workers as local processes</li>
            <li><input type="checkbox" disabled checked /> Remove the <code>Spawner</code> trait altogether
              <ul class="list-bullet">
                <li>It&#39;s no longer necessary and isn&#39;t sufficiently expressive for most spawners</li>
              </ul>            </li>
          </ul>        </li>
      </ul>    </li>
    <li><h2>Test Datasets</h2>
      <ul class="list-bullet">
        <li><a href="https://opensky-network.org/datasets/raw/">https://opensky-network.org/datasets/raw/</a></li>
      </ul>    </li>
    <li><h2>Initial Features</h2>
      <ul class="list-bullet">
        <li>Split up queries across a number of workers
          <ul class="list-bullet">
            <li>Split queries into chunks</li>
            <li>Max number of concurrent workers</li>
          </ul>        </li>
        <li>Run workers</li>
        <li>Gather results</li>
        <li>Reduce results using 1 or more reducers</li>
        <li>Retryable workers with some threshold for retrying
          <ul class="list-bullet">
            <li>e.g. autoretry remaining workers once the first 90% have returned</li>
          </ul>        </li>
      </ul>    </li>
    <li><h2>Pluggable Backends</h2>
      <ul class="list-bullet">
        <li>Execution
          <ul class="list-bullet">
            <li>AWS Lambda</li>
            <li>Docker/Kubernetes/Nomad</li>
          </ul>        </li>
        <li>Storage
          <ul class="list-bullet">
            <li>Various Cloud storage</li>
          </ul>        </li>
      </ul>    </li>
    <li><h2>Applications</h2>
      <ul class="list-bullet">
        <li>Query over some data and output a DuckDB file with the results for further analysis.</li>
      </ul>    </li>
    <li><h3>From Honeycomb&#39;s O11y Book

But serverless functions and cloud object storage aren&#39;t 100% reliable. In practice, the latency at the tails of the distribution of invocation time can be significant orders of magnitude higher than the median time. That last 5% to 10% of results may take tens of seconds to return, or may never complete. In the Retriever implementation, we use impatience to return results in a timely manner. Once 90% of the requests to process segments have completed, the remaining 10% are re-requested, without canceling the still-pending requests. The parallel attempts race each other, with whichever returns first being used to populate the query result. Even if it is 10% more expensive to always retry the slowest 10% of subqueries, a different read attempt against the cloud provider&#39;s backend will likely perform faster than a &quot;stuck&quot; query blocked on S3, or network I/O that may never finish before it times out.


What happens if someone attempts to group results by a high-cardinality field? How can you still return accurate values without running out of memory? The simplest solution is to fan out the reduce step by assigning reduce workers to handle only a proportional subset of the possible groups. For instance, you could follow the pattern Chord does by creating a hash of the group and looking up the hash correspondence in a ring covering the keyspace.</h3></li>
  </ul>

