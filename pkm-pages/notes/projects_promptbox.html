---
title: "PromptBox"
tags: Projects
date: 2023-10-31
updated: 2023-12-12
---


  <ul class="list-bullet">
    <li>This utility allows maintaining libraries of LLM prompt templates which can be filled in and submitted from the command line.</li>
    <li><a href="https://github.com/dimfeld/promptbox">Github</a>, <a href="https://promptbox.imfeld.dev">Website</a></li>
    <li>A sample prompt. Each of the options below becomes a CLI flag which can fill in the template.
      <ul class="list-bullet">
        <li><pre><code><span class="sy-source sy-toml"><span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">description</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>Summarize some files<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span>

<span class="sy-comment sy-line sy-number-sign sy-toml"><span class="sy-punctuation sy-definition sy-comment sy-toml">#</span> This can also be template_path to read from another file.</span>
<span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">template</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-triple sy-literal sy-block sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&#39;&#39;&#39;</span>
Create a {{style}} summary of the below files
which are on the topic of {{topic}}. The summary should be about {{ len }} sentences long.

{% for f in file -%}
File {{ f.filename }}:
{{ f.contents }}


{%- endfor %}
<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&#39;&#39;&#39;</span></span>

<span class="sy-punctuation sy-definition sy-table sy-begin sy-toml">[</span><span class="sy-meta sy-tag sy-table sy-toml"><span class="sy-entity sy-name sy-table sy-toml">model</span></span><span class="sy-punctuation sy-definition sy-table sy-end sy-toml">]</span>
<span class="sy-comment sy-line sy-number-sign sy-toml"><span class="sy-punctuation sy-definition sy-comment sy-toml">#</span> These model options can also be defined in a config file to apply to the whole directory of templates.</span>
<span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">model</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>gpt-3.5-turbo<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span>
<span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">temperature</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-constant sy-numeric sy-float sy-toml">0.7</span>
<span class="sy-comment sy-line sy-number-sign sy-toml"><span class="sy-punctuation sy-definition sy-comment sy-toml">#</span> Also supports top_p, frequency_penalty, presence_penalty, stop, and max_tokens</span>

<span class="sy-punctuation sy-definition sy-table sy-begin sy-toml">[</span><span class="sy-meta sy-tag sy-table sy-toml"><span class="sy-entity sy-name sy-table sy-toml">options</span></span><span class="sy-punctuation sy-definition sy-table sy-end sy-toml">]</span>
<span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">len</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-punctuation sy-definition sy-inline-table sy-begin sy-toml">{</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">type</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>int<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span><span class="sy-punctuation sy-separator sy-inline-table sy-toml">,</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">description</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>The length of the summary<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span><span class="sy-punctuation sy-separator sy-inline-table sy-toml">,</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">default</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-constant sy-numeric sy-integer sy-toml">4</span> <span class="sy-punctuation sy-definition sy-inline-table sy-end sy-toml">}</span>
<span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">topic</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-punctuation sy-definition sy-inline-table sy-begin sy-toml">{</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">type</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>string<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span><span class="sy-punctuation sy-separator sy-inline-table sy-toml">,</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">description</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>The topic of the summary<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span> <span class="sy-punctuation sy-definition sy-inline-table sy-end sy-toml">}</span>
<span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">style</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-punctuation sy-definition sy-inline-table sy-begin sy-toml">{</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">type</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>string<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span><span class="sy-punctuation sy-separator sy-inline-table sy-toml">,</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">default</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>concise<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span> <span class="sy-punctuation sy-definition sy-inline-table sy-end sy-toml">}</span>
<span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">file</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-punctuation sy-definition sy-inline-table sy-begin sy-toml">{</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">type</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>file<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span><span class="sy-punctuation sy-separator sy-inline-table sy-toml">,</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">array</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-constant sy-language sy-toml">true</span><span class="sy-punctuation sy-separator sy-inline-table sy-toml">,</span> <span class="sy-meta sy-tag sy-key sy-toml"><span class="sy-entity sy-name sy-tag sy-toml">description</span></span> <span class="sy-punctuation sy-definition sy-key-value sy-toml">=</span> <span class="sy-string sy-quoted sy-double sy-basic sy-toml"><span class="sy-punctuation sy-definition sy-string sy-begin sy-toml">&quot;</span>The files to summarize<span class="sy-punctuation sy-definition sy-string sy-end sy-toml">&quot;</span></span> <span class="sy-punctuation sy-definition sy-inline-table sy-end sy-toml">}</span>
</span></code></pre></li>
      </ul>    </li>
    <li><h2>Task List</h2>
      <ul class="list-bullet">
        <li><h3>Up Next</h3>
          <ul class="list-bullet">
            <li><input type="checkbox" readonly="true"  /> Pass images to Ollama
              <ul class="list-bullet">
                <li>Once <a href="https://github.com/jmorganca/ollama/pull/1216">Multimodal support by pdevine · Pull Request #1216 · jmorganca/ollama</a> is merged and released</li>
              </ul>            </li>
            <li><input type="checkbox" readonly="true"  /> Support for GPT4 Vision
              <ul class="list-bullet">
                <li>Can <a href="https://platform.openai.com/docs/guides/vision/uploading-base-64-encoded-images">pass images in base64</a></li>
              </ul>            </li>
          </ul>        </li>
        <li><h3>Soon</h3>
          <ul class="list-bullet">
            <li><input type="checkbox" readonly="true"  /> Verbose mode should print token stats at end</li>
            <li><input type="checkbox" readonly="true"  /> List command
              <ul class="list-bullet">
                <li>List all templates in a directory</li>
                <li>should also take a filter</li>
                <li>Short mode for completion</li>
                <li>by default print template name and description in a table</li>
              </ul>            </li>
            <li><input type="checkbox" readonly="true"  /> Show command to output the information from a template</li>
            <li><input type="checkbox" readonly="true"  /> &quot;run&quot; command detection is fragile</li>
          </ul>        </li>
        <li><h3>Later/Maybe</h3>
          <ul class="list-bullet">
            <li><input type="checkbox" readonly="true"  /> Save all invocations in a database?</li>
            <li><input type="checkbox" readonly="true"  /> Allow templates to reference partials in same directory</li>
            <li><input type="checkbox" readonly="true"  /> Allow templates to reference partials in parent template directories</li>
            <li><input type="checkbox" readonly="true"  /> Define ChatGPT functions in the prompt? Probably skip this, more appropriate for some other project</li>
            <li><input type="checkbox" readonly="true"  /> bash/zsh Autocomplete template names</li>
            <li><input type="checkbox" readonly="true"  /> Can we autocomplete options as well once the template name is present?</li>
            <li><input type="checkbox" readonly="true"  /> Recall previous invocations</li>
            <li><input type="checkbox" readonly="true"  /> Option to trim context in the middle with a <code>&lt;truncated&gt;</code> message or something like that</li>
          </ul>        </li>
        <li><h3>Done</h3>
          <ul class="list-bullet">
            <li><input type="checkbox" readonly="true" checked /> Support <a href="https://openrouter.ai/">OpenRouter</a> &mdash; v0.2.0 Dec 8th, 2023
              <ul class="list-bullet">
                <li>OpenRouter offers an OpenAI compatible API which is probably the easiest way to add this.</li>
              </ul>            </li>
            <li><input type="checkbox" readonly="true" checked /> Set prompt format, context length, etc. per model - v0.2.0 Dec 8th, 2023
              <ul class="list-bullet">
                <li>Done specifically for Together right now, can expand this to generic host at some point</li>
                <li>Support standard formats and allow custom formats too</li>
                <li>Needed for some providers that don&#39;t apply the template for you, or who don&#39;t provide accurate info about context length and other things.</li>
                <li>This can be defined in the model definition once it can be an object (see <a class="block-ref" href="/notes/projects_promptbox#656e51d7-5951-46f1-9945-f52622163231"><input type="checkbox" readonly="true" checked /> Support multiple hosts - v0.2.0 Dec 8th, 2023</a>).</li>
              </ul>            </li>
            <li><input type="checkbox" readonly="true" checked /> Support together.xyz model host - v0.2.0 Dec 8th, 2023
              <ul class="list-bullet">
                <li>Fetch model info from <code>https://api.together.xyz/models/info</code></li>
                <li>Short term cache on the model info</li>
                <li>Get the config from the model info to determine how to format the prompt, stop tokens, etc.
                  <ul class="list-bullet">
                    <li>Some of the configs here actually don&#39;t include things like system prompt...
                      <ul class="list-bullet">
                        <li>Maybe just build in templates to the tool and lets them be specified in the config somehow</li>
                      </ul>                    </li>
                    <li>Looks like context length is missing from some models as well</li>
                  </ul>                </li>
                <li>max_tokens seems have a very small default, need to set this higher to be useful</li>
              </ul>            </li>
            <li id="656e51d7-5951-46f1-9945-f52622163231"><input type="checkbox" readonly="true" checked /> Support multiple hosts - v0.2.0 Dec 8th, 2023
              <ul class="list-bullet">
                <li><input type="checkbox" readonly="true" checked /> Allow defining hosts beyond the built-in hosts
                  <ul class="list-bullet">
                    <li>API url</li>
                    <li>Request Format (currently just OpenAI and Ollama format but probably more in the future)</li>
                    <li>Environment variable that holds the API key</li>
                  </ul>                </li>
                <li><input type="checkbox" readonly="true" checked /> Ability to configure the default host for non-GPT-3.5/4 models (whereas now Ollama is the default)</li>
                <li><input type="checkbox" readonly="true" checked /> Need a way to specify in the model name which host to use
                  <ul class="list-bullet">
                    <li>Actually the way to do this is to allow the model name to be either a string or a <code>{ host: Option&lt;String&gt;, model: String }</code> structure.</li>
                  </ul>                </li>
                <li><input type="checkbox" readonly="true" checked /> Tests
                  <ul class="list-bullet">
                    <li><input type="checkbox" readonly="true" checked /> config file overrides specific fields of bulit-in hosts
                      <ul class="list-bullet">
                        <li>e.g. <code>host.openai.api_key = &quot;DIFFERENT_VAR_NAME&quot;</code></li>
                      </ul>                    </li>
                    <li><input type="checkbox" readonly="true" checked /> Adding new hosts from the config file</li>
                    <li><input type="checkbox" readonly="true" checked /> Use default provider when none is specified</li>
                    <li><input type="checkbox" readonly="true" checked /> Set <code>default_host</code> to something else</li>
                    <li><input type="checkbox" readonly="true" checked /> Complain when default_host refers to a nonexistent host</li>
                    <li><input type="checkbox" readonly="true" checked /> Alias handling
                      <ul class="list-bullet">
                        <li>Alias can be a full model spec</li>
                        <li>Model can be a full model spec and also reference an alias which is a full model spec. Should fetch the alias from <code>model</code> and merge the remaining fields together in this case</li>
                      </ul>                    </li>
                  </ul>                </li>
              </ul>            </li>
            <li><input type="checkbox" readonly="true" checked /> Testing
              <ul class="list-bullet">
                <li><input type="checkbox" readonly="true" checked /> stop at the top_level config</li>
                <li><input type="checkbox" readonly="true" checked /> Resolution of model options between different configs</li>
                <li><input type="checkbox" readonly="true" checked /> Don&#39;t require a config in every directory</li>
                <li><input type="checkbox" readonly="true" checked /> Malformed configs raise an error</li>
                <li><input type="checkbox" readonly="true" checked /> Malformed templates throw an error</li>
                <li><input type="checkbox" readonly="true" checked /> templates resolved in order from the current directory</li>
                <li><input type="checkbox" readonly="true" checked /> Look under <code>./promptbox.toml</code> and <code>./promptbox/promptbox.toml</code></li>
                <li><input type="checkbox" readonly="true" checked /> Prompts can be in subdirectories</li>
                <li><input type="checkbox" readonly="true" checked /> Prepend</li>
                <li><input type="checkbox" readonly="true" checked /> Append</li>
                <li><input type="checkbox" readonly="true" checked /> Prepend and append</li>
                <li><input type="checkbox" readonly="true" checked /> all types of arguments</li>
                <li><input type="checkbox" readonly="true" checked /> Bool arguments are always optional</li>
                <li><input type="checkbox" readonly="true" checked /> required arguments (switch from required to optional)</li>
                <li><input type="checkbox" readonly="true" checked /> Array arguments</li>
                <li><input type="checkbox" readonly="true" checked /> Template model options should override config model options</li>
                <li><input type="checkbox" readonly="true" checked /> make sure it works to invoke with command-line options and template options at the same time</li>
                <li><input type="checkbox" readonly="true" checked /> system prompt, embedded and in separate file</li>
                <li><input type="checkbox" readonly="true" checked /> json mode</li>
              </ul>            </li>
            <li><input type="checkbox" readonly="true" checked /> Handle 429 from OpenAI &mdash; v0.1.2 Dec 4th, 2023</li>
            <li><input type="checkbox" readonly="true" checked /> Chop off too-large context, option to keep beginning or end &mdash; v0.1.1 Dec 1st, 2023
              <ul class="list-bullet">
                <li>Should also be able to specify which inputs to slice off i.e. keep the fixed template intact but remove some of the piped-in input</li>
                <li>Ideally have per-model context values.
                  <ul class="list-bullet">
                    <li>Ollama can get this from the API.</li>
                    <li>OpenAI has few enough models that we can do some basic pattern matching to make a guess</li>
                    <li>But need ability to specify a lower cap too, e.g. maybe we never actually want to send 128K tokens to GPT4</li>
                  </ul>                </li>
              </ul>            </li>
            <li><input type="checkbox" readonly="true" checked /> Token counter functionality &mdash; v0.1.1 Nov 30th, 2023</li>
            <li><input type="checkbox" readonly="true" checked /> Set up CI and distribution &mdash; Nov 21th, 2023</li>
            <li><input type="checkbox" readonly="true" checked /> Streaming support for openai &mdash; Nov 14th, 2023</li>
            <li><input type="checkbox" readonly="true" checked /> Append any additional positional arguments</li>
            <li><input type="checkbox" readonly="true" checked /> Append input from stdin</li>
            <li><input type="checkbox" readonly="true" checked /> Support <code>format=&quot;json&quot;</code></li>
            <li><input type="checkbox" readonly="true" checked /> Streaming support for ollama</li>
            <li><input type="checkbox" readonly="true" checked /> Integrate with <a href="https://github.com/jmorganca/ollama">ollama</a></li>
            <li><input type="checkbox" readonly="true" checked /> Option type to paste a file contents in, and allow wildcards for array files</li>
            <li><input type="checkbox" readonly="true" checked /> Send request to model</li>
            <li><input type="checkbox" readonly="true" checked /> Move the main command to be a &quot;run&quot; subcommand</li>
            <li><input type="checkbox" readonly="true" checked /> Basic functionality</li>
            <li><input type="checkbox" readonly="true" checked /> Define CLI options in template file</li>
            <li><input type="checkbox" readonly="true" checked /> Help output always shows openai_key (maybe due to .env?)</li>
          </ul>        </li>
      </ul>    </li>
  </ul>

