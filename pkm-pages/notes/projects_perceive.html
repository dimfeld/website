---
title: "Perceive"
tags: Projects
date: 2022-12-21
updated: 2023-10-11
---


  <ul class="list-bullet">
    <li>This project was an experiment from December 2022, in indexing a bunch of personal data and performing semantic search on it via embedding similarity. Everything on the backend was done in Rust just to make it hard. :)</li>
    <li>Tauri app
      <ul class="list-bullet">
        <li>Loading bar for initial load</li>
        <li>Search field</li>
        <li>real-time search as you type (without highlighting)</li>
        <li>ability to select different sources, source categories</li>
      </ul>    </li>
    <li>Asymmetric Search
      <ul class="list-bullet">
        <li>This is useful when we have an asymmetric query, where the queries are short but the corpuses to be matched are long.</li>
        <li>Probably not worrying about this for now, maybe later</li>
        <li><a href="https://www.sbert.net/examples/applications/retrieve_rerank/README.html">https://www.sbert.net/examples/applications/retrieve_rerank/README.html</a></li>
        <li>Use tokenizer: <a href="https://docs.rs/rust-bert/latest/rust_bert/pipelines/sentence_embeddings/struct.SentenceEmbeddingsTokenizerConfigResources.html#associatedconstant.ALL_MINI_LM_L12_V2">https://docs.rs/rust-bert/latest/rust_bert/pipelines/sentence_embeddings/struct.SentenceEmbeddingsTokenizerConfigResources.html#associatedconstant.ALL_MINI_LM_L12_V2</a>
          <ul class="list-bullet">
            <li>Possible additional config? <a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2/blob/main/tokenizer_config.json">https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2/blob/main/tokenizer_config.json</a></li>
            <li>The conversion process might include all this, not sure.</li>
          </ul>        </li>
        <li>Implementing a Cross-Encoder
          <ul class="list-bullet">
            <li>Unlike the normal bi-encoder process, which generates vectors from one document at a time and then relies on cosine similarity and other metrics, the cross-encoder model takes both documents together and produces a similarity score straight from the model.</li>
            <li>This can give much better results when you have a top list of similar documents, but want to sort that list further for presentation to the user. It&#39;s much slower though, which is why you use the vector similar from the bi-encoder to get the set of candidate documents.</li>
            <li>We do a normal compare on the bi-encoder vectors to get a top N, and the re-sort that results list with the CrossEncoder, which creates an encoding of the large result and the query together, and then gets the top score from each one.</li>
            <li><a href="https://www.sbert.net/examples/applications/cross-encoder/README.html">https://www.sbert.net/examples/applications/cross-encoder/README.html</a></li>
            <li><a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/cross_encoder/CrossEncoder.py">https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/cross_encoder/CrossEncoder.py</a></li>
          </ul>        </li>
        <li>SBERT provides pre-trained cross encoder models from the MS MARCO dataset
          <ul class="list-bullet">
            <li><a href="https://www.sbert.net/docs/pretrained_cross-encoders.html">https://www.sbert.net/docs/pretrained_cross-encoders.html</a></li>
            <li>Data at <a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2/tree/main">https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2/tree/main</a></li>
            <li>Need to convert the model weights and see what else needs to happen for this to work.</li>
          </ul>        </li>
        <li>Other Cross-Encoder Models
          <ul class="list-bullet">
            <li>ColBERT - <a href="https://huggingface.co/vespa-engine/col-minilm">https://huggingface.co/vespa-engine/col-minilm</a> - Colbert uses a lighter weight context model to speed things up</li>
            <li><a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2">https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2</a></li>
          </ul>        </li>
      </ul>    </li>
    <li>rust-bert on M1
      <ul class="list-bullet">
        <li>1. The latest published version of the <code>rust-bert</code> crate is using <code>tch-rs </code>0.8, but you need to use at least 0.10 instead. The git version already uses the newest version, so you can just set that in your  Cargo.toml: <code>rust-bert = { git = &quot;[github.com/guillaume-be/rust-b](https://github.com/guillaume-be/rust-bert.git)&quot; }</code></li>
        <li>2. You need to install pytorch manually. The simplest method is to just install it globally via Homebrew or pip3, though a more robust method would be to install a local copy in a python venv or something, and reference it from there.</li>
        <li>3. Set the LIBTORCH environment variable to wherever you have libtorch installed. With the homebrew method this is <code>LIBTORCH=/opt/homebrew/opt/pytorch</code></li>
        <li>4. Tell the linker where to find it in your Cargo config.</li>
      </ul>    </li>
    <li><h2>Initial Sprint Reflections</h2>
      <ul class="list-bullet">
        <li>Still much easier to do ML stuff in Python, though rust-bert was hugely useful in implementing a lot of this</li>
        <li>Setting up libtorch</li>
        <li>Web scraping browser history is kind of a hassle
          <ul class="list-bullet">
            <li>Lots of pages require authentication</li>
            <li>Github really hates it, even for public pages</li>
            <li>Content extraction for HTML
              <ul class="list-bullet">
                <li>Readability works well, rust port needs some work</li>
                <li>In the future would probably run a sidecar that hosts the up-to-date JS version</li>
              </ul>            </li>
          </ul>        </li>
        <li>Rayon thread pool exhaustion</li>
        <li>Model choice matters a lot</li>
        <li>Running bulk inference without GPU support is still slow for the larger models.</li>
        <li>ndarray is great</li>
        <li>Future work
          <ul class="list-bullet">
            <li>Better article scraping</li>
            <li>ML model feedback</li>
            <li>More integrations</li>
            <li>OpenAI integration to allow running on less powerful systems</li>
          </ul>        </li>
      </ul>    </li>
  </ul>

